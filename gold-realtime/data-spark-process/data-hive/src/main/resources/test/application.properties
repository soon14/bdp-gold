#============================spark-core配置项================================
#设置spark Streaming 应用程序的名字
spark.commons.appName = hiveEtlBatch

#设置系统打印日记的水平
spark.commons.logLevel= WARN


#设置spark程序的序列化方式(默认java自带)
spark.serializer = org.apache.spark.serializer.KryoSerializer
#内存用来存储的比例,这部分内存中有多少可以用于Memory Store管理RDD Cache的数据
spark.storage.memoryFraction = 0.2
#是否开启调整内存存储比例功能
spark.memory.useLegacyMode = true
#set user memory在整个 executor memory 的占比。（默认 0.6 -> 0.2)，也可调大 executor memory来增加 user memory
spark.memory.fraction = 0.6
#checkpoint的路径和目录(暂定必填)
spark.commons.checkpoint = hdfs://nameservice-standby/user/sparksql/hiveEtlBatch/checkpoint


#============================spark-shuffle================================
#sparkStreaming.Shuffle.manager的类型(hash,sort)
spark.shuffle.manager = sort
#spark.shuffle缓存大小,默认32K 内存充足可以调到64,减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数
spark.shuffle.file.buffer = 64k
#spark shuffle read 拉取属于自己数据时重试的次数
spark.shuffle.io.maxRetries = 3
#spark shuffle read 拉取属于自己数据时重试的间隔
spark.shuffle.io.retryWait = 5s
#spark集群中每个executor中用来shuffle的等操作的内存大小最小比例。shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘
spark.shuffle.memoryFraction = 0.6


#=============================spark-sql====================================
#大小写转义
spark.sql.caseSensitive = true
spark.sql.user.db = use fox_data

#===============================hive=======================================
#动态分区
hive.exec.dynamic.partition = true
hive.exec.dynamic.partition.mode = nonstrict
hive.exec.compress.output = true

hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.merge.mapfiles = true
hive.merge.mapredfiles = true
hive.merge.size.per.task = 1024000000
hive.merge.smallfiles.avgsize = 1024000000
mapred.max.split.size = 2048000000


#================================mysql=====================================
#链接mysql的jdbcURL
mysql.jdbc.url =jdbc:mysql://bdata-test-mysql.server.fjf:3306/fox-data-bi?useUnicode=true&characterEncoding=utf8&useSSL=false
#披量写mysql的批量大小
mysql.connect.batch.size = 1000
#连接mysql的用户名
mysql.jdbc.username=root
#连接mysql的密码
mysql.jdbc.password=123456
#jdbc driver
mysql.jdbc.driver=com.mysql.jdbc.Driver


#==================================App===================================
app.mode.standalone=false
app.root.path=/spark-task